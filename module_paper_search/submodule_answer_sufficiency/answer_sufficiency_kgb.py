"""
KGB-based answer sufficiency evaluation module.

This module evaluates the sufficiency of answers generated by the KGB-based
answer generation module and provides feedback for improvement.
"""

import os
import json
import logging
import requests
import re
from typing import Dict, Any, Tuple
from pathlib import Path

# Import the save_answer function from the answer generation module
from ..submodule_answer_generation.answer_generation_kgb import save_answer

from .answer_sufficiency_kgb_parameters import (
    EVALUATION_MODEL,
    EVALUATION_MAX_TOKENS,
    EVALUATION_TEMPERATURE,
    EVALUATION_SYSTEM_PROMPT,
    GRADE_OUTPUT,
    GRADE_MIN,
    EVALUATION_JSON_PATH,
    EVALUATION_TXT_PATH
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def evaluate_answer(answer: str, enhanced_query: str) -> Tuple[float, str]:
    """
    Evaluate the quality of an answer using the GPT-4-Turbo model.
    
    Args:
        answer: The answer to evaluate
        enhanced_query: The enhanced query that was used to generate the answer
        
    Returns:
        Tuple of (grade, explanation)
    """
    logger.info("Evaluating answer quality...")
    
    # Prepare the user prompt
    user_prompt = f"""
Question:
{enhanced_query}

Answer to evaluate:
{answer}

Please evaluate this answer based on the criteria provided.
"""
    
    # Call the evaluation model
    grade, explanation = call_evaluation_model(user_prompt)
    
    # Save evaluation results
    save_evaluation(grade, explanation, answer, enhanced_query)
    
    logger.info(f"Answer evaluation completed with grade: {grade}")
    return grade, explanation


def call_evaluation_model(prompt: str) -> Tuple[float, str]:
    """
    Call the evaluation model to get a grade and explanation.
    
    Args:
        prompt: The prompt for the evaluation model
        
    Returns:
        Tuple of (grade, explanation)
    """
    # Prepare API request
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}"
    }
    
    data = {
        "model": EVALUATION_MODEL,
        "messages": [
            {"role": "system", "content": EVALUATION_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        "temperature": EVALUATION_TEMPERATURE,
        "max_tokens": EVALUATION_MAX_TOKENS
    }
    
    # Call API
    try:
        logger.info(f"Calling evaluation model: {EVALUATION_MODEL}")
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data
        )
        response.raise_for_status()
        
        # Parse response
        result = response.json()
        evaluation_text = result['choices'][0]['message']['content']
        
        # Extract grade from the response
        grade_match = re.search(r'Rating:\s*(\d+\.\d+)', evaluation_text)
        if grade_match:
            grade = float(grade_match.group(1))
        else:
            logger.warning("Could not extract grade from evaluation response, defaulting to 0.0")
            grade = 0.0
        
        # Extract explanation (everything after "Explanation:")
        explanation_match = re.search(r'Explanation:(.*)', evaluation_text, re.DOTALL)
        if explanation_match:
            explanation = explanation_match.group(1).strip()
        else:
            explanation = "No explanation provided."
        
        logger.info(f"Evaluation completed with grade: {grade}")
        return grade, explanation
        
    except Exception as e:
        logger.error(f"Error calling evaluation model: {e}")
        return 0.0, f"Error during evaluation: {str(e)}"


def save_evaluation(grade: float, explanation: str, answer: str, enhanced_query: str) -> None:
    """
    Save the evaluation results to JSON and TXT files.
    
    Args:
        grade: The evaluation grade
        explanation: The explanation for the grade
        answer: The evaluated answer
        enhanced_query: The enhanced query
    """
    # Prepare evaluation data
    evaluation_data = {
        "grade": grade,
        "explanation": explanation,
        "answer": answer,
        "enhanced_query": enhanced_query,
        "timestamp": logging.Formatter.formatTime(logging.Formatter(), logging.LogRecord("", 0, "", 0, None, None, None))
    }
    
    # Save to JSON
    try:
        with open(EVALUATION_JSON_PATH, 'w') as f:
            json.dump(evaluation_data, f, indent=2)
        logger.info(f"Saved evaluation to JSON: {EVALUATION_JSON_PATH}")
    except Exception as e:
        logger.error(f"Error saving evaluation to JSON: {e}")
    
    # Save to TXT
    try:
        with open(EVALUATION_TXT_PATH, 'w') as f:
            f.write(f"QUERY:\n{enhanced_query}\n\n")
            f.write(f"ANSWER:\n{answer}\n\n")
            f.write(f"EVALUATION:\nGrade: {grade}\n\n{explanation}")
        logger.info(f"Saved evaluation to TXT: {EVALUATION_TXT_PATH}")
    except Exception as e:
        logger.error(f"Error saving evaluation to TXT: {e}")


def run_answer_sufficiency_kgb(answer: str, enhanced_query: str, nth_loop: int, max_loops: int, queries_list: list[str]) -> Tuple[str, bool, float, list[str]]:
    """
    Run the KGB-based answer sufficiency evaluation and determine if the answer is sufficient.
    
    Args:
        answer: The answer to evaluate
        enhanced_query: The enhanced query that was used to generate the answer
        nth_loop: The current loop number (0-based)
        max_loops: The maximum number of loops
        queries_list: The list of queries used to generate the answer
        
    Returns:
        Tuple of (final_answer, is_sufficient, grade, queries_list)
    """
    logger.info(f"Running KGB-based answer sufficiency evaluation (loop {nth_loop+1}/{max_loops})...")
    
    # Evaluate the answer
    grade, explanation = evaluate_answer(answer, enhanced_query)
    
    # Determine if the answer is sufficient
    is_sufficient = False
    final_answer = answer
    
    if grade >= GRADE_OUTPUT:
        # Answer is excellent, accept it
        logger.info(f"Answer grade {grade} >= {GRADE_OUTPUT}, accepting as sufficient")
        is_sufficient = True
        final_answer = answer
    elif grade >= GRADE_MIN and nth_loop == max_loops - 1:
        # Answer meets minimum requirements and we're at the last loop
        logger.info(f"Answer grade {grade} >= {GRADE_MIN} and at final loop, accepting as sufficient")
        is_sufficient = True
        final_answer = answer
    elif grade < GRADE_MIN and nth_loop == max_loops - 1:
        # Answer doesn't meet minimum requirements and we're at the last loop
        logger.warning(f"Answer grade {grade} < {GRADE_MIN} at final loop, returning failure message")
        is_sufficient = False
        final_answer = (
            "I couldn't find a satisfactory answer to your question based on the available research papers. "
            "The information available may be insufficient or not directly relevant to your specific query. "
            "Consider refining your question or exploring related topics that might have more research coverage."
        )
        
        # Save the failure message to the answer_kgb.json and answer_kgb.txt files
        logger.info("Saving failure message to output files")
        try:
            # Create an empty list for selected_chunks since we don't have any for the failure message
            save_answer(final_answer, enhanced_query, [])
            logger.info("Successfully saved failure message to output files")
        except Exception as e:
            logger.error(f"Error saving failure message to output files: {e}")
        
    else:
        # Answer needs improvement, continue to next loop
        if nth_loop == 0:
            # Extract only the first and last elements from queries_list (i.e. subtopic and topic queries)
            queries_list = [queries_list[0], queries_list[-1]] if len(queries_list) > 1 else queries_list
        logger.info(f"Answer grade {grade} needs improvement, continuing to next loop")
        is_sufficient = False
        final_answer = answer
    
    return final_answer, is_sufficient, grade, queries_list
